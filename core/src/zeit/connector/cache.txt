Caches
======

Body cache
++++++++++

We are caching the bodies of resources. The cache removes objects a given time
after the last access, by default 30 days.

>>> import zeit.connector.cache
>>> cache = zeit.connector.cache.ResourceCache()

>>> cache.UPDATE_INTERVAL = 0.9


Validation
----------

For getting data from the cache, the cache needs the etag from the properties:

>>> properties = {('getetag', 'DAV:'): 'hurz'}

Initially the cache is empty and asking for a non  cached entry yields a
KeyError:

>>> cache.getData('some-id', properties)
Traceback (most recent call last):
    ...
KeyError: "Object 'some-id' is not cached."


Set some data. `setData` returns either a StringIO or an open file handle. What
it returns depends on the size of the input data. For small data is returned as
StringIO:

>>> from io import BytesIO
>>> data = BytesIO(b'datadata/end of line.')
>>> f = cache.setData('some-id', properties, data)
>>> f
<...BytesIO object at 0x...>
>>> f.read()
b'datadata/end of line.'
>>> f.close()

We can get the data also via `getData` now which also returns a StringIO for
small objects:

>>> f = cache.getData('some-id', properties)
>>> f
<...BytesIO object at 0x...>
>>> f.close()

We have data in the cache. Since it's a small object it is stored as a
reference to a plain string:

>>> from zeit.connector.cache import get_storage_key
>>> cached = cache._data[get_storage_key('some-id')]
>>> cached
<zeit.connector.cache.Body object at 0x...>
>>> cached.data
b'datadata/end of line.'
>>> f = cached.open('r')
>>> f
<...BytesIO object at 0x...>
>>> f.read()
b'datadata/end of line.'
>>> f.close()

Also the access times are noted in the cache:

>>> last_access = cache._last_access_time[get_storage_key('some-id')]
>>> isinstance(last_access, (int, int))
True
>>> cache._access_time_to_ids[last_access]
<BTrees.OIBTree.OITreeSet object at 0x...>
>>> list(cache._access_time_to_ids[last_access])
[b'some-id']

When we now access getData the access time will be updated by at most one
count since less time than the update interval should have passed:

>>> f = cache.getData('some-id', properties)
>>> f
<...BytesIO object at 0x...>
>>> f.close()
>>> cache._last_access_time[get_storage_key('some-id')] <= last_access + 1
True

Let's wait 1 second:

>>> import time
>>> time.sleep(1)

If we access now the time will be updated:

>>> f = cache.getData('some-id', properties)
>>> f
<...BytesIO object at 0x...>
>>> f.close()
>>> new_access = cache._last_access_time[get_storage_key('some-id')]
>>> new_access > last_access
True

Also the the time to id mapping has been updated:

>>> list(cache._access_time_to_ids[last_access])
[]
>>> cache._access_time_to_ids[new_access]
<BTrees.OIBTree.OITreeSet object at 0x...>
>>> list(cache._access_time_to_ids[new_access])
[b'some-id']

Removal of cached bodies by time out
------------------------------------

Add another resource to the cache to see how resources are removed from the
cache:

>>> data = BytesIO(b'another text')
>>> cache.setData('another-id', properties, data).close()

We now have two elements in the cache:

>>> sorted(get_storage_key(k) for k in cache._last_access_time.keys())
[b'another-id', b'some-id']

Now sleep another 4 second and see the `some-id` will be gone after sweeping:

>>> time.sleep(1)
>>> cache.sweep(4)
>>> f = cache.getData('another-id', properties)
>>> f
<...BytesIO object at 0x...>
>>> f.close()
>>> sorted(get_storage_key(k) for k in cache._last_access_time.keys())
[b'another-id', b'some-id']

Still there ...

>>> time.sleep(1)
>>> cache.sweep(4)
>>> f = cache.getData('another-id', properties)
>>> f
<...BytesIO object at 0x...>
>>> f.close()
>>> sorted(get_storage_key(k) for k in cache._last_access_time.keys())
[b'another-id', b'some-id']

Still there ...

>>> time.sleep(1)
>>> cache.sweep(4)
>>> f = cache.getData('another-id', properties)
>>> f
<...BytesIO object at 0x...>
>>> f.close()
>>> sorted(get_storage_key(k) for k in cache._last_access_time.keys())
[b'another-id', b'some-id']

Now it's gone:

>>> time.sleep(1)
>>> cache.sweep(4)
>>> f = cache.getData('another-id', properties)
>>> f
<...BytesIO object at 0x...>
>>> f.close()
>>> sorted(get_storage_key(k) for k in cache._last_access_time.keys())
[b'another-id']

We don't have any data stored for the key now:

>>> cache._data[get_storage_key(b'some-id')]
Traceback (most recent call last):
    ...
KeyError: ...


Large data handling
-------------------

For large objects we store a blob. What a large object is, is determined by the
buffer size:

>>> BUFFER_SIZE = zeit.connector.cache.Body.BUFFER_SIZE

Create data which is larger than that:


>>> large_data = 'Large' + ('Data' * BUFFER_SIZE)
>>> large_data = bytes(large_data.encode('utf-8'))
>>>
>>> f = cache.setData('some-id', properties, BytesIO(large_data))
>>> f.read() == large_data
True
>>> f.close()

We can get the data also via `getData` which returns an open file handle:

>>> f = cache.getData('some-id', properties)
>>> hasattr(f, 'read')
True
>>> f.close()

Internally a blob is created in this case:

>>> body = cache._data[get_storage_key('some-id')]
>>> body
<zeit.connector.cache.Body object at 0x...>
>>> body.data
<ZODB.blob.Blob object at 0x...>

When the transaction is commited we'll get a file directly from the blob
storage. Make the blob commited:

>>> import transaction
>>> transaction.commit()

We get the commited data now:

>>> f = cache.getData('some-id', properties)
>>> f.read() == large_data
True
>>> f.close()

Transaction integration
-----------------------

When a resource is just read from the cache, no part of the cache needs to be
written to the database (given that no timeouts had to be updated).

Create a new cache and attach it to the database root:

>>> cache = zeit.connector.cache.ResourceCache()
>>> getRootFolder()['cache'] = cache
>>> transaction.commit()

Add an object to the cache and commit:

>>> data = BytesIO(b'Frodo went deep into Mordor.')
>>> properties = {('getetag', 'DAV:'): 'ent'}
>>> cache.setData('mordor', properties, data).close()
>>> transaction.commit()

After commiting everything is "not changed" of course. Now get an the object
from the cache:

>>> f = cache.getData('mordor', properties)
>>> f.close()

All the cache parts are still not changed:

>>> cache._p_changed
False
>>> cache._data._p_changed
False
>>> cache._last_access_time._p_changed
False
>>> cache._access_time_to_ids._p_changed
False
>>> [s for s in cache._access_time_to_ids.values() if s._p_changed]
[]


Conflict resolution
-------------------

Body conflict resolution works by comparing etags. When conflicting etags are
different the result state is "not cached". When the etags are equal, the state
is preserved.

Note that we don't handle the case where the etags are equal but the bodies are
different. We don't expect this to happen.

>>> from ZODB import DB
>>> import ZODB.blob
>>> import tempfile
>>> import zc.queue.tests
>>> blob_dir = tempfile.mkdtemp()
>>> storage = zc.queue.tests.ConflictResolvingMappingStorage('test')
>>> storage = ZODB.blob.BlobStorage(blob_dir, storage)
>>> db = DB(storage)
>>> transactionmanager_1 = transaction.TransactionManager()
>>> transactionmanager_2 = transaction.TransactionManager()
>>> connection_1 = db.open(transaction_manager=transactionmanager_1)
>>> root_1 = connection_1.root()

>>> c_1 = root_1["cache"] = zeit.connector.cache.ResourceCache()
>>> body = b'A small file' * BUFFER_SIZE
>>> c_1.setData('id', {('getetag', 'DAV:'): 'initial-etag'}, BytesIO(body)).close()
>>> transactionmanager_1.commit()

>>> transactionmanager_2 = transaction.TransactionManager()
>>> connection_2 = db.open(transaction_manager=transactionmanager_2)
>>> root_2 = connection_2.root()
>>> c_2 = root_2['cache']

Different ETAGS:

>>> body1 = BytesIO(b'Body 1' * BUFFER_SIZE)
>>> body2 = BytesIO(b'Body 2' * BUFFER_SIZE)
>>> c_1.setData('id', {('getetag', 'DAV:'): 'etag1'}, body1).close()
>>> c_2.setData('id', {('getetag', 'DAV:'): 'etag2'}, body2).close()
>>> transactionmanager_2.commit()
>>> transactionmanager_1.commit()
>>> connection_1.sync()
>>> connection_2.sync()
>>> c_1.getData('id', {('getetag', 'DAV:'): 'etag1'})
Traceback (most recent call last):
    ...
KeyError: "Object 'id' is not cached."
>>> c_2.getData('id', {('getetag', 'DAV:'): 'etag2'})
Traceback (most recent call last):
    ...
KeyError: "Object 'id' is not cached."

The same ETAG also doesn't generate a conflict:

>>> c_1.setData('id', {('getetag', 'DAV:'): 'etag3'}, BytesIO(body)).close()
>>> c_2.setData('id', {('getetag', 'DAV:'): 'etag3'}, BytesIO(body)).close()
>>> transactionmanager_2.commit()
>>> transactionmanager_1.commit()
>>> connection_1.sync()
>>> connection_2.sync()
>>> f = c_1.getData('id', {('getetag', 'DAV:'): 'etag3'})
>>> f.read()
b'A small fileA small...'
>>> f.close()
>>> f = c_2.getData('id', {('getetag', 'DAV:'): 'etag3'})
>>> f.read()
b'A small fileA small...'
>>> f.close()


Property Cache
++++++++++++++

We need to manually clear the key cache from previous tests runs due to
isolation problems:

>>> zeit.connector.cache.WebDAVPropertyKey._instances.clear()

The property cache stores all the webdav proprties:

>>> cache = zeit.connector.cache.PropertyCache()

Validate the interface:

>>> import zope.interface.verify
>>> zope.interface.verify.verifyObject(
...     zeit.connector.interfaces.IPersistentCache,
...     cache)
True

Test the methods:

>>> cache['id'] = {('ns', 'foo'): 'bar'}
>>> dict(cache['id'])
{<WebDAVPropertyKey ('ns', 'foo')>: 'bar'}
>>> dict(cache.get('id'))
{<WebDAVPropertyKey ('ns', 'foo')>: 'bar'}
>>> cache.get('foo', "don't have that")
"don't have that"

>>> 'id' in cache
True
>>> 'blafasel' in cache
False
>>> list(cache.keys())
[b'id']

>>> del cache['id']
>>> cache['id']
Traceback (most recent call last):
    ...
KeyError: 'id'

>>> cache.get('id') is None
True
>>> list(cache.keys())
[]

Note that there is actually still the entry in the cache, but marked as
deleted:

>>> list(cache.keys(include_deleted=True))
[b'id']

To really remove there is the remove method, which also works for "deleted"
keys:

>>> cache.remove('id')
>>> list(cache.keys(include_deleted=True))
[]

The property cache does some magic to prevent extensive memory consumption. The
keys in the cache *values* are rather few, maybe 200. So it keeps track of the
different keys and stores/returns references to already passed ones.

>>> key = ('ns', 'name')
>>> properties = {key: 'somevalue'}
>>> cache['id'] = properties

Store another property set with another instance of "key":

>>> key2 = ('ns', 'name')
>>> key == key2
True
>>> key is key2
False

>>> cache['otherid'] = {key2: 'othervalue'}

When we get the properties of 'otherid' back, the key2 is the same instance as
key:

>>> key2_back = list(cache['otherid'].keys())[0]
>>> key2_back == key2 == key
True
>>> key2_back is key2
False

We can also directly set keys on the property dict:

>>> cache['id'][('foo', 'bar')] = 'zoot'
>>> cache['id'][('foo', 'bar')]
'zoot'

Note that the key is also munged into a WebDAVPropertyKey:

>>> sorted(list(cache['id'].keys()))
[<WebDAVPropertyKey ('foo', 'bar')>, <WebDAVPropertyKey ('ns', 'name')>]

Removing also works directly:

>>> del cache['id'][('foo', 'bar')]

The cache keeps track of the used keys:

>>> import pprint
>>> pprint.pprint(zeit.connector.cache.WebDAVPropertyKey._instances)
{('foo', 'bar'): <WebDAVPropertyKey ('foo', 'bar')>,
 ('ns', 'foo'): <WebDAVPropertyKey ('ns', 'foo')>,
 ('ns', 'name'): <WebDAVPropertyKey ('ns', 'name')>}

When the a value is set to the cache which is equal to the value it alreay
stores, the new value is not updated. We verify this by setting a new value
which is equal but not the same:

>>> dict(cache['id'])
{<WebDAVPropertyKey ('ns', 'name')>: 'somevalue'}
>>> new_properties =  {('ns', 'name'): 'somevalue'}

Setting ``new_properties`` does not update the ``somevalue`` to unicode:

>>> cache['id'] = new_properties
>>> dict(cache['id'])
{<WebDAVPropertyKey ('ns', 'name')>: 'somevalue'}


Note that internally no dictionaries are stored but a special object which also
does conflict resolution:

>>> cache._storage[b'id']
<zeit.connector.cache.Properties object at 0x...>



Conflict resolution
-------------------

XXX This section is obsolete and not used in production anymore
>>> zeit.connector.cache.FEATURE_TOGGLES.set(
...     'dav_cache_delete_property_on_conflict', value=True)

The property cache resolves conflicts in a very brutal way as it considers the
cache stale when ever there was a conflict. Since it is only a cache we do not
loose any data here.

Note that the most common conflicts are value changes *in* the cache. We do not
change the conflict resolution of the underling btree.

>>> c_1 = root_1["cache"] = zeit.connector.cache.PropertyCache()
>>> c_1['id'] = {('ns', 'foo'): 'bar'}
>>> transactionmanager_1.commit()

>>> transactionmanager_2 = transaction.TransactionManager()
>>> connection_2 = db.open(transaction_manager=transactionmanager_2)
>>> root_2 = connection_2.root()
>>> c_2 = root_2['cache']

>>> c_1['id'] = {('ns', 'foo'): 'baz'}
>>> c_2['id'] = {('ns', 'oink'): 'chackalacka'}
>>> transactionmanager_2.commit()
>>> transactionmanager_1.commit()
>>> connection_1.sync()
>>> connection_2.sync()
>>> c_1['id']
Traceback (most recent call last):
    ...
KeyError: 'id'
>>> c_2['id']
Traceback (most recent call last):
    ...
KeyError: 'id'


When the conflicting state is *equal* it is kept, though:

>>> c_1['id'] = {('ns', 'foo'): 'baz'}
>>> c_2['id'] = {('ns', 'foo'): 'baz'}
>>> transactionmanager_2.commit()
>>> transactionmanager_1.commit()
>>> connection_1.sync()
>>> connection_2.sync()
>>> dict(c_1['id'])
{<WebDAVPropertyKey ('ns', 'foo')>: 'baz'}
>>> dict(c_2['id'])
{<WebDAVPropertyKey ('ns', 'foo')>: 'baz'}


The conflict resolution ignores the property {INTERNAL}cached-time when
determining if two property sets are equal. This is fairly save because:

1. The cached-time is only relevant when the resource is locked.

2. When the resource is locked, the lock timeout is indicated in seconds from
   the cached time. When the cached times differ much, the lock timeout will
   differ. Thus the properties are not equal.

Note that the cached time of the transaction wins which is commited last:

>>> import datetime
>>> import pytz
>>> c_1['id'][('cached-time', 'INTERNAL')] = datetime.datetime(
...     2008, 1, 1, tzinfo=pytz.UTC).isoformat()
>>> c_2['id'][('cached-time', 'INTERNAL')] = datetime.datetime(
...     2009, 4, 7, tzinfo=pytz.UTC).isoformat()
>>> transactionmanager_2.commit()
>>> transactionmanager_1.commit()
>>> connection_1.sync()
>>> connection_2.sync()
>>> pprint.pprint(dict(c_1['id']))
{<WebDAVPropertyKey ('cached-time', 'INTERNAL')>: '2008-01-01T00:00:00+00:00',
 <WebDAVPropertyKey ('ns', 'foo')>: 'baz'}
>>> pprint.pprint(dict(c_2['id']))
{<WebDAVPropertyKey ('cached-time', 'INTERNAL')>: '2008-01-01T00:00:00+00:00',
 <WebDAVPropertyKey ('ns', 'foo')>: 'baz'}


When there was an attribute set on the cache entry, it cannot be resolved any
longer:

>>> c_1['id'].foo = 'bar'
>>> c_2['id'][('hick', 'hack')] = 'huck'
>>> transactionmanager_2.commit()
>>> transactionmanager_1.commit()
Traceback (most recent call last):
    ...
ConflictError: database conflict error...

>>> transactionmanager_1.abort()

Child name cache
++++++++++++++++

The childname cache just stores a mapping of unique id to a list of child
names:

>>> cache = zeit.connector.cache.ChildNameCache()
>>> cache['id'] = ['foo', 'bar', 'baz']

The data is returned as a ChildNames object which also sorts the ids:

>>> cache['id']
<zeit.connector.cache.ChildNames object at 0x...>
>>> list(cache['id'])
['bar', 'baz', 'foo']

When we remove things from the cache they're gone:

>>> del cache['id']
>>> cache['id']
Traceback (most recent call last):
    ...
KeyError: 'id'

>>> cache.get('id') is None
True

But actually the're just marked as deleted:

>>> cache._storage[b'id']
<zeit.connector.cache.ChildNames object at 0x...>
>>> list(cache._storage[b'id'])
[DeleteProperty]


When the a value is set to the cache which is equal to the value it alreay
stores, the new value is not updated. We verify this by setting a new value
which is equal but not the same:

>>> cache['id'] = ['foo', 'bar', 'baz']
>>> new_children =  ['foo', 'bar', 'baz']

Setting ``new_children`` does not update ``baz`` to unicode:

>>> cache['id'] = new_children
>>> list(cache['id'])
['bar', 'baz', 'foo']

Conflict resolution
-------------------

XXX This section is obsolete and not used in production anymore
>>> zeit.connector.cache.FEATURE_TOGGLES.set(
...     'dav_cache_delete_childname_on_conflict', value=True)

The conflict resolution works like the property cache:

>>> c_1 = root_1["cache"] = zeit.connector.cache.ChildNameCache()
>>> c_1['id'] = ('foo', 'bar')
>>> transactionmanager_1.commit()

>>> transactionmanager_2 = transaction.TransactionManager()
>>> connection_2 = db.open(transaction_manager=transactionmanager_2)
>>> root_2 = connection_2.root()
>>> c_2 = root_2['cache']

>>> c_1['id'] = ('baz',)
>>> c_2['id'] = ('oink', 'chackalacka')
>>> transactionmanager_2.commit()
>>> transactionmanager_1.commit()
>>> connection_1.sync()
>>> connection_2.sync()
>>> c_1['id']
Traceback (most recent call last):
    ...
KeyError: 'id'
>>> c_2['id']
Traceback (most recent call last):
    ...
KeyError: 'id'

When the conflicting state is *equal* it is kept, though:

>>> c_1['id'] = ('foo',)
>>> c_2['id'] = ('foo',)
>>> transactionmanager_2.commit()
>>> transactionmanager_1.commit()
>>> connection_1.sync()
>>> connection_2.sync()
>>> list(c_1['id'])
['foo']
>>> list(c_2['id'])
['foo']


Clean up
++++++++

>>> import shutil
>>> shutil.rmtree(blob_dir)
